{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82\n"
     ]
    }
   ],
   "source": [
    "import os #for access directory and folder of operating system\n",
    "import numpy as np\n",
    "import cv2 #for image conversion means convert into pixle\n",
    "import matplotlib.pyplot as plt #for show image that are converted into the array of pixel\n",
    "training_data=[] #Traing data will store into that list\n",
    "category = [\"darshak\",\"saurabh\",\"thiren\"] \n",
    "directory = 'C:/Users/om/Desktop/'\n",
    "for typee in category:\n",
    "    path = os.path.join(directory,typee) #show path = \"C:/Users/om/Desktop/darshak\"\n",
    "    class_name = category.index(typee) #show index of the catesgory like darshak=0,saurabh=1\n",
    "    for img in os.listdir(path):\n",
    "        try:\n",
    "            img_array = cv2.imread(os.path.join(path,img),cv2.IMREAD_GRAYSCALE) #convert jpg to pixle of array and also convert into grayscale...........\n",
    "            new_array = cv2.resize(img_array,(50,50)) #50,50 -> convert size of image into 50*50 pixel\n",
    "            training_data.append([new_array,class_name]) #append that pixle of array of image(image),index of category(label) into the traiing_data list means [[darshak_image,0],[saurabh_image,1]]..........\n",
    "        except Exception as e:\n",
    "            pass\n",
    "print(len(training_data)) #Total number of input images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random #for shuffle all the traiing_input array\n",
    "random.shuffle(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "Y = []\n",
    "f=open(\"lable.txt\", \"a\")\n",
    "for features,label in training_data:\n",
    "    X.append(features)\n",
    "    Y.append(label)\n",
    "    f.write('%d\\n' % label)\n",
    "X = np.array(X).reshape(-1,50,50,1) # 1-> here image is gray scale so write 1 , if ur image is rgb then write  3.\n",
    "Y = np.array(Y)\n",
    "\n",
    "X = X/255.0 #Reduce pixle size\n",
    "print(Y[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:\n",
      "\n",
      "  TensorFlow's `tf-nightly` package will soon be updated to TensorFlow 2.0.\n",
      "\n",
      "  Please upgrade your code to TensorFlow 2.0:\n",
      "    * https://www.tensorflow.org/beta/guide/migration_guide\n",
      "\n",
      "  Or install the latest stable TensorFlow 1.X release:\n",
      "    * `pip install -U \"tensorflow==1.*\"`\n",
      "\n",
      "  Otherwise your code may be broken by the change.\n",
      "\n",
      "  \n",
      "WARNING:tensorflow:From E:\\anaconda\\lib\\site-packages\\tensorflow_core\\python\\ops\\resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape=(50, 50, 1))) # 32->number of nuerons, (3,3)->window size of input, (50,50,1)->image size 50*50 and 1 because image is gray scale....\n",
    "model.add(Activation('relu')) #Activation function\n",
    "model.add(MaxPooling2D(pool_size=(2, 2))) #MaxPooling2D is layer which select maximum size window and combine into new 2*2 array so pool_size is (2,2).......\n",
    "\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# the model so far outputs 3D feature maps (height, width, features)\n",
    "\n",
    "model.add(Flatten())# this converts our 3D feature maps to 1D feature vectors\n",
    "\n",
    "model.add(Dense(64)) # (64)->nuerons in Dense layer\n",
    "model.add(Activation('relu'))\n",
    "\n",
    "# model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3)) # final 1 output layer\n",
    "model.add(Activation('softmax'))\n",
    "# COMPILEself\n",
    "model.compile(loss='sparse_categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])#sparse_categorycal_crossentropy=if more than one class,categorycal_crossentropy=if predict only one class\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 73 samples, validate on 9 samples\n",
      "Epoch 1/15\n",
      "73/73 [==============================] - 2s 27ms/sample - loss: 1.1235 - acc: 0.3699 - val_loss: 1.0966 - val_acc: 0.2222\n",
      "Epoch 2/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 1.0872 - acc: 0.3973 - val_loss: 1.0785 - val_acc: 0.2222\n",
      "Epoch 3/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 1.0538 - acc: 0.5616 - val_loss: 1.0370 - val_acc: 0.4444\n",
      "Epoch 4/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.9911 - acc: 0.5753 - val_loss: 0.9612 - val_acc: 0.6667\n",
      "Epoch 5/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.9159 - acc: 0.5342 - val_loss: 1.0877 - val_acc: 0.5556\n",
      "Epoch 6/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.9493 - acc: 0.5616 - val_loss: 0.9869 - val_acc: 0.4444\n",
      "Epoch 7/15\n",
      "73/73 [==============================] - ETA: 0s - loss: 0.7735 - acc: 0.716 - 0s 3ms/sample - loss: 0.7877 - acc: 0.7260 - val_loss: 1.0443 - val_acc: 0.5556\n",
      "Epoch 8/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.8270 - acc: 0.6164 - val_loss: 0.9884 - val_acc: 0.5556\n",
      "Epoch 9/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.7203 - acc: 0.6575 - val_loss: 0.9015 - val_acc: 0.6667\n",
      "Epoch 10/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.6449 - acc: 0.8082 - val_loss: 1.2532 - val_acc: 0.4444\n",
      "Epoch 11/15\n",
      "73/73 [==============================] - 0s 5ms/sample - loss: 0.6665 - acc: 0.6712 - val_loss: 1.1353 - val_acc: 0.4444\n",
      "Epoch 12/15\n",
      "73/73 [==============================] - 0s 5ms/sample - loss: 0.6117 - acc: 0.7534 - val_loss: 1.1575 - val_acc: 0.5556\n",
      "Epoch 13/15\n",
      "73/73 [==============================] - 0s 5ms/sample - loss: 0.5230 - acc: 0.7945 - val_loss: 1.1716 - val_acc: 0.4444\n",
      "Epoch 14/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.4158 - acc: 0.8767 - val_loss: 1.0129 - val_acc: 0.5556\n",
      "Epoch 15/15\n",
      "73/73 [==============================] - 0s 4ms/sample - loss: 0.3121 - acc: 0.9041 - val_loss: 1.2814 - val_acc: 0.4444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1e31460b630>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X,Y,epochs=15,batch_size=5,validation_split=0.1) #batch_size=total number of input pass at a time, validation_split=(0.1)10% of input are use for validation......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import tensorflow as tf\n",
    "def convert_image(img):\n",
    "    ia = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "    na = cv2.resize(ia,(50,50))\n",
    "    return na.reshape(-1,50,50,1)\n",
    "\n",
    "t  = np.array(convert_image('C:/Users/om/Desktop/2.jpg'))\n",
    "t = tf.cast(t, tf.float32)\n",
    "p = model.predict(t,steps=1)\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "save_model() missing 1 required positional argument: 'filepath'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-8fbfb43e12bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Save tf.keras model in HDF5 format.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mkeras_file\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"myimage_pred.h5\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: save_model() missing 1 required positional argument: 'filepath'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Save tf.keras model in HDF5 format.\n",
    "keras_file = \"myimage_pred.h5\"\n",
    "tf.keras.models.save_model(keras_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'call'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-7967d760e90c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# Convert to TensorFlow Lite model.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m# from tensorflow.contrib import lite\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mconverter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlite\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTFLiteConverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_keras_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeras_file\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mtflite_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconverter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"myimage__pred.tflite\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"wb\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtflite_model\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\lite\\python\\lite.py\u001b[0m in \u001b[0;36mfrom_keras_model\u001b[1;34m(cls, model)\u001b[0m\n\u001b[0;32m    380\u001b[0m       \u001b[0mTFLiteConverter\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    381\u001b[0m     \"\"\"\n\u001b[1;32m--> 382\u001b[1;33m     \u001b[0mfunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_saving_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrace_model_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    383\u001b[0m     \u001b[0mconcrete_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_concrete_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconcrete_func\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tensorflow\\lib\\site-packages\\tensorflow_core\\python\\keras\\saving\\saving_utils.py\u001b[0m in \u001b[0;36mtrace_model_call\u001b[1;34m(model, input_signature)\u001b[0m\n\u001b[0;32m    121\u001b[0m   \"\"\"\n\u001b[0;32m    122\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0minput_signature\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 123\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdef_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFunction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    124\u001b[0m       \u001b[0minput_signature\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minput_signature\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    125\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'call'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# Convert to TensorFlow Lite model.\n",
    "# from tensorflow.contrib import lite\n",
    "converter = tf.lite.TFLiteConverter.from_keras_model(keras_file)\n",
    "tflite_model = converter.convert()\n",
    "open(\"myimage__pred.tflite\", \"wb\").write(tflite_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
